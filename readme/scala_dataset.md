
# Social Media Post Analysis with Spark DataFrames

This project demonstrates how to analyze semi-structured JSON data representing social media posts using Spark DataFrames. The goal is to extract meaningful insights from data related to the Russian presidential election.

## Project Overview

This project focuses on analyzing data collected from the VK social network during the Russian presidential election. The analysis includes:

*   **Data Loading and Preparation:** Loading JSON data into a Spark DataFrame and handling schema variations.
*   **Data Exploration:** Determining the frequency of optional attributes and validating assumptions about data dependencies.
*   **Analysis of Posts by Type:** Calculating the number of posts for each event type (e.g., comment, share).
*   **Tag Analysis:** Analyzing the frequency of different tags used in the posts.
*   **Fact-Checking with Wikipedia:** Comparing tag usage with candidate vote counts from Wikipedia to identify potential biases.
*   **Creation of Data Cubes:** Building a cube to aggregate posts across different dimensions (tag, event type, month).
*   **Co-occurrence Matrix of Tags:** Identifying relationships between different tags based on the authors who use them.

## Data Sources

The project uses the following data sources:

*   **VKRU18s.tgz:** A gzipped archive containing JSON data representing social media posts from the VK social network during the Russian presidential election.  You will need to extract this archive to the `/dataset/VKRU18s/` directory.
*   **Wikipedia:** Candidate vote counts are obtained from Wikipedia for fact-checking.

## Implementation

The `scala_dataset1.py` script uses Spark DataFrames to perform the data analysis tasks.

### 1. Data Loading and Preparation

The script loads the JSON data into a Spark DataFrame:

```python
# Initialize Spark session
spark = SparkSession.builder.appName("VKAnalysis").getOrCreate()

# Set the path to the data
path = "/dataset/VKRU18s/"
vk_file = path + "vk_001.json"

# Load the data, remove duplicates, and calculate total records
data = spark.read.format("json").load(vk_file).dropDuplicates()
total = data.count()
```

### 2. Data Exploration

To understand the structure and contents of the data it is important to explore and check what types of data are being processed.

```python
# Print the schema
data.printSchema()
```

Now what the goal is to look to which of the schema is optional and which is not. For this we have

```python
def optional_vs_obligatory():
    """Calculates and displays the frequency of optional attributes."""

    attrs = ["event","event.event_id","event.event_id.post_id","event.event_id.post_owner_id","event.event_id.comment_id","event.event_id.shared_post_id","event.author","event.attachments","event.geo","event.tags","event.creation_time"]
    attrs_freq = [data.where(col(x).isNotNull()).count() for x in attrs]
    print('Frequence of each columns:')
    for i in range(len(attrs)):
        print("Frequency of " + attrs[i] + ' is: ' + str(attrs_freq[i]))

    #Checking whether an attribute that the event_id exists only if the event type is comment

    comment_check = data.where("event.event_type='comment'").where("event.event_id.comment_id is null").count()
    print('Checking that if the event type is comment the comment ID must not be null : ' +str(comment_check))
    shared_check = data.where("event.event_type='share'").where("event.event_id.shared_post_id is null").count()
    print('Checking that if the event type is shared_post the shared_post_id must not be null : ' +str(shared_check))
```

The code first defines a list of attributes, then uses the `where` function along with `is not null` to only count non-null values

### 3. Number of Posts per Event Type

The next step is to calculate number of posts per event Type, first extract each of the values then group by the data to extract the counts:

```python
postPerType = data.select("event.event_type","event.event_id.post_id").distinct().groupBy("event_type").count()

print('Posts counts per event type: ')
postPerType.show()
```

### 4. Flattening the List of Tags

```python
dataWithTags = data.withColumn("tag", explode(col("event.tags")))
```

This code uses the explode function to convert the array that contains the tag to a dataset

### 5. Compute the number of posts per tag

```python
# group data to make calculate the number of distinct posts per tag
postsPerTag = dataWithTags.groupBy("tag").count().orderBy(col("count").desc())
print('Number of posts per tag: ')
postsPerTag.show()
```

The data is then grouped by the values generated by tag, which are then ordered

### 6. Calculating the number of distinct authors per tag

```python
authCountPerTag =dataWithTags.groupBy("tag").agg(expr("count(_id)").alias("nbAuths"))
authCountPerTag.orderBy(col("nbAuths").desc).show()
```

To do this we use the `agg` function which enables us to specify an expression that counts the values

### 7. Wikipedia Fact Checking

This portion uses Wikipedia data about the votes for each candidate to see if the tag is correlated with the number of authors that used it.

First load the data from Wikipedia, create the schema then display:

```python
#Loading the data from Wikipedia, to be able to show the credibility of the data

from pyspark.sql import Row
from pyspark.sql.types import StructType, StructField, StringType, LongType

# Define the schema for the votes
schema = StructType([
    StructField("name", StringType(), True),
    StructField("party", StringType(), True),
    StructField("votes", LongType(), True)
])

# Create the data
data2 = [
    Row("putin", "Independent", 56430712),
    Row("grudinin", "Communist", 8659206),
    Row("zhirinovsky","Liberal Democratic Party",4154985),
    Row("sobchak","Civic Initiative",1238031),
    Row("yavlinsky","Yabloko",769644),
    Row("titov","Party of Growth",556801)
]

votes = spark.createDataFrame(data2, schema)
votes.printSchema()
votes.show()
```
Then, using the following the counts of votes for each of the authors is calculated using the following instructions
```python
votesCount = votes.join(authCountPerTag,votes["name"] == authCountPerTag["tag"]).orderBy(col("votes").desc).select("name","votes","nbAuths")
votesCount.show()
```

It is interesting to note that the `count` command was never used in order to count the total rows

Now to validate all the calculations and show them with a table the following has to be run

```python
from pyspark.sql.functions import col, from_unixtime

windowSpec = Window.orderBy(desc("votes"))

# Adding ranks and checking
ranked = votesCount.withColumn("votes_rank",rank().over(Window.orderBy(col("votes").desc()))).withColumn("nbAuths_rank",rank().over(Window.orderBy(col("nbAuths").desc())))
ranked.show()
```

### 8. Add the creation month from which data has been created

```python
#Add a 'month' attribute
dataTagMon = dataWithTags.withColumn("month",from_unixtime(col("event.creation_time"),"M"))
dataTagMon.show()
```

### 9. The roleup
What this functions does is:
*   First it groups with all distincts attributes
*   Then filters for the null values
*   The filters out the other data we don't want to work with.

```python
#Rollup aggregates data at multiple levels, from the most detailed to the most summarized, based on the specified columns
cub_ev_tag_mo =  dataTagMon.rollup("event.event_type", "tag", "month").count()
cub_ev_tag_mo_notnull = cub_ev_tag_mo.where("event_type is not null and tag is not null and month is not null")
cub_ev_tag_mo_notnull.orderBy(col("count").desc).show()
```

### 10 Create a Cross table
```python
#Group the counts based on event type and the month, to create counts for all dimensions in that group

monthEvent = cub_ev_tag_mo_notnull.groupBy("event_type","month")
monthEvent.printSchema
monthEvent.show

#Show the pivoted table to indicate a co-relation between the counts per mont
monthEvent = cub_ev_tag_mo_notnull.groupBy("month").pivot("event_type").sum("count")
monthEvent.show()
```

### 11 The co-occurrence matrix

```python
# Get all tags used by a single author
authTag = dataWithTags.select("event.author.id","tag").distinct()

# Join tag author with the tags and rename a column
dataWithTags_a = dataWithTags.withColumnRenamed("event", "event1").withColumnRenamed("tag", "otherTag")

# get the data from the tags
TupletagsPerAuthor_a = authTag.join(dataWithTags_a,authTag.tag == dataWithTags_a.otherTag).where(f"authTag.id = dataWithTags_a.event1.author.id and tag > otherTag")

# Now group by the author, the number of posts with those tags and show the number of occurences and name it as count
tagCoOcc = TupletagsPerAuthor_a.groupBy("tag", "otherTag").agg(size(collect_list(authTag.col("event1.author.id"))).alias("count"))
tagCoOcc.show(30)

# Now group by the number of other tag and take the new sum for each one, and name it count
tagMat = tagCoOcc.groupBy("tag").pivot("otherTag").agg(sum("count").alias("count"))
tagMat.show()
```

This portion shows the co-occurrences matrix that is calculated as follows:

*   The tags used by each of the authours
*   Then join data with the previous step
*   And calculates the number of posts per tag, giving more insights about the dataset

## Conclusion

This project provides a comprehensive example of data analysis using Spark DataFrames. It covers a wide range of operations, from basic data loading and filtering to more advanced techniques.
